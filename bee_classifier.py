# -*- coding: utf-8 -*-
"""bee classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11QNnEW57RN2XpV0T_waoaKYVtYYOdVV0
"""

from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt # plotting
import numpy as np # linear algebra
import os # accessing directory structure
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import tensorflow 
import glob
from PIL import Image
import sys
from pathlib import Path
import imageio
import skimage
from skimage import io
import skimage.transform
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from google.colab import drive
from sklearn.preprocessing import OneHotEncoder
from keras.utils import to_categorical
from keras.models import Sequential
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPooling2D

drive.mount('/content/drive')

IMAGE_PATH = "/content/drive/MyDrive/Colab Notebooks/Annotated Honey Bees/The BeeImage Dataset Annotated Honey Bee Images/bee_imgs/bee_imgs/"

image_files = list(os.listdir(IMAGE_PATH))
print("Number of image files: {}".format(len(image_files)))

image_files[0]

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

import cv2
import glob


imagefile = glob.glob('/content/drive/MyDrive/Colab Notebooks/Annotated Honey Bees/The BeeImage Dataset Annotated Honey Bee Images/bee_imgs/bee_imgs/*')

len(imagefile)

imgages = []

j = 1
for i in imagefile:
   # img.append(cv2.imread(i,cv2.IMREAD_GRAYSCALE))
   img = cv2.imread(i)
   img2 = cv2.resize(img,(64,64))
   imgages.append(img2)

plt.imshow(imgages[1000])

from google.colab import drive
drive.mount('/content/drive')

plt.imshow(imgages[99])

bee_data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Annotated Honey Bees/The BeeImage Dataset Annotated Honey Bee Images/bee_data.csv",dtype={'subspecies':'category'})

print(bee_data.head())

bee_data.subspecies.value_counts().plot.bar(title="Subspecies count in dataset")
plt.ylabel("Count")
plt.show()
bee_data.subspecies.value_counts()

print(bee_data.columns)

bee_data.drop(labels=["date", "time", "location", "zip code", "health", "pollen_carrying", "caste"],axis=1)

y = bee_data['subspecies']

def process_data():
    """ This function just links the images files with subspecies and returns train dataframe and test dataframe
        args : takes no argument 
        returns : two dataframes """
    df = pd.DataFrame()
    df.insert(0, "Y", y)
    df.insert(0, "img", imgages)
    (train, test) = train_test_split(df, test_size=0.25, random_state=42)
    return (train, test)

def numpyfy(df):
  """this function truns the images into numpy array"""
    arr = []
    df_numpy = df.to_numpy()
    
    for i in df_numpy:
        arr.append(i)
    arr = np.asarray(arr)
    #print(arr.shape), returns 4 dimensional array
    return arr

train, test = process_data()

print(train.head())



train_img, test_img = numpyfy(train["img"]), numpyfy(test["img"])

print(train_img.shape)
print(test_img.shape)

train_y = np.array(train['Y'])
test_y = np.array(test['Y'])
print(train_y.shape)

train_y = np.reshape(train_y, (-1,1))
test_y = np.reshape(test_y, (-1,1))

# define one hot encoding
encoder = OneHotEncoder(sparse=False)
# transform data
y_train = encoder.fit_transform(train_y)
y_test = encoder.fit_transform(test_y)
print(y_train.shape)
print(y_test.shape)

X_train = train_img / 255
X_test = test_img / 255

print(y_test.shape)
print(y_train.shape)
print(X_test.shape)
print(X_train.shape)

aug = ImageDataGenerator( rotation_range=20,zoom_range=0.15,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.15,horizontal_flip=True,fill_mode="nearest")

# setting model constraints
num_classes = 7
# defining a sequential model
model_1 =Sequential()
model_1.add(Conv2D(64, kernel_size=(3,3),activation='relu',input_shape=(64,64,3)))
model_1.add(MaxPooling2D(pool_size=(2,2)))
model_1.add(Dropout(0.1))
model_1.add(Conv2D(64, kernel_size=(3,3),activation='relu'))
model_1.add(MaxPooling2D(pool_size=(2,2)))
model_1.add(Dropout(0.1))
model_1.add(Conv2D(128,kernel_size=(3,3),activation='relu'))
model_1.add(MaxPooling2D(pool_size=(2,2)))
model_1.add(Dropout(0.1))
model_1.add(Flatten())
model_1.add(Dense(128, activation='relu'))
model_1.add(Dropout(0.2))
model_1.add(Dense(64, activation='relu'))
model_1.add(Dropout(0.2))
model_1.add(Dense(num_classes, activation='sigmoid', name='preds'))
model_1.summary()

model_1.compile(loss="categorical_crossentropy",optimizer='adam', metrics=['accuracy'])
model_1.fit(x=aug.flow(X_train, y_train, batch_size=64),epochs=25,verbose=1,validation_data=(X_test,y_test))

score = model_1.evaluate(X_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

print("")

model_1.save("annotatedHoneyBeemodel.h5")